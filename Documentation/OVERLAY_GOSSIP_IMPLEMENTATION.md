## Overlay Gossip ‚Äî Implementaci√≥n y gu√≠a

Fecha: 18 de noviembre de 2025

Este documento resume los cambios realizados para implementar un prototipo de descubrimiento distribuido basado en gossip (overlay) y su integraci√≥n con el cliente BitTorrent existente en este repositorio. Explica los componentes nuevos, c√≥mo probar localmente, limitaciones actuales y pasos siguientes recomendados.

## Objetivo

Proveer una alternativa al tracker centralizado: un overlay distribuido que permite `Announce` y `Lookup` mediante gossip (propagaci√≥n epid√©mica). No se usa DHT tradicional; la soluci√≥n replica informaci√≥n de proveedores (peers) por infohash entre nodos.

## Archivos nuevos y cambios principales

1. Nuevo paquete `src/overlay` (implementaci√≥n propia):
   - `src/overlay/store.go`
     - `Store` en memoria que mantiene `infoHash -> providers`.
     - TTL para providers stale (por defecto 90 segundos).
     - M√©todos: `NewStore`, `Announce`, `Merge`, `Lookup`, `ToJSON`.
     - Estructura `ProviderMeta{Addr, PeerId, Left, LastSeen}`.

   - `src/overlay/gossip.go`
     - `Overlay` controla el store, mantiene lista bootstrap y un TCP listener JSON.
     - Mensajes JSON por TCP: `gossip`, `announce`, `lookup`.
     - Periodic gossip (cada ~8s) que hace full-push de providers por infohash a peers bootstrap.
     - M√©todos: `NewOverlay`, `Start`, `Stop`, `Announce`, `Lookup`.

2. Cambios en `src/client/cmd/main.go` (integraci√≥n cliente):
   - Nuevos flags:
     - `--discovery-mode` (tracker|overlay) ‚Äî por defecto `tracker`.
     - `--bootstrap` lista de bootstrap peers (comma-separated, `host:port`).
     - `--overlay-port` puerto TCP donde escucha overlay (default `6000`).
   - Si `--discovery-mode=overlay`:
     - Se inicia el `overlay` (listener TCP JSON y gossip peri√≥dico).
     - En vez de anunciar por HTTP al tracker, se llama `ov.Announce(infoHash, ProviderMeta)`.
     - En vez de parsear `peers` del tracker, se llama `ov.Lookup(infoHash, limit)` para obtener peers.
     - Los announces peri√≥dicos, `completed`, y `stopped` se env√≠an al overlay si est√° activo.
   - Modo `tracker` mantiene comportamiento original.

3. Otros: no se modificaron los paquetes `peerwire` fundamentales excepto su uso por el cliente.

## Dise√±o del overlay (resumen t√©cnico)

- Estructura de datos: por cada `infoHash` se guarda un map de `addr -> ProviderMeta`. Merge es LWW sobre `LastSeen`.
- Propagaci√≥n: cada nodo env√≠a peri√≥dicamente (full-push) la lista de providers por `infoHash` a sus peers bootstrap.
- Lookup: retorna providers locales ordenados por `LastSeen` y pregunta a hasta 3 peers bootstrap para completar la respuesta.
- TTL: providers con `LastSeen` anterior a `now - TTL` se ignoran en Lookup. TTL por defecto 90s.

Racional: dise√±o simple, resistente a churn moderado, evita DHT y permite pol√≠ticas avanzadas luego (bloom filters, diffs, r√©plica adaptativa).

## C√≥mo probar (ejemplos)

Compilar (desde el directorio `src`):

```bash
cd /home/jabel/Documentos/GIthub_repo/BitTorrent/src
go build ./...
```

Ejecutar tres instancias locales (puedes usar `go run` para pruebas r√°pidas). Cambia rutas al `.torrent` y `--archives` seg√∫n corresponda:

Terminal A (overlay puerto 6000):

```bash
go run ./client/cmd --torrent="../video.torrent" --archives="~/jabel/Documentos/GIthub_repo/BitTorrent/src" \
  --discovery-mode=overlay --overlay-port=6000 --bootstrap="127.0.0.1:6001,127.0.0.1:6002" --hostname=127.0.0.1
```

Terminal B (overlay puerto 6001):

```bash
go run ./client/cmd --torrent="../video.torrent" --archives="~/jabel/Documentos/GIthub_repo/BitTorrent/src" \
  --discovery-mode=overlay --overlay-port=6001 --bootstrap="127.0.0.1:6000,127.0.0.1:6002" --hostname=127.0.0.1
```

Terminal C (overlay puerto 6002):

```bash
go run ./client/cmd --torrent="../video.torrent" --archives="~/jabel/Documentos/GIthub_repo/BitTorrent/src" \
  --discovery-mode=overlay --overlay-port=6002 --bootstrap="127.0.0.1:6000,127.0.0.1:6001" --hostname=127.0.0.1
```

- Observa la salida: cada nodo imprimir√° "Overlay gossip iniciado" y "Announced to overlay, left=...".
- Tras algunos ciclos (~8s cada uno) deber√≠as observar que otros nodos obtienen providers para ese `infoHash` mediante `Lookup` y podr√°n conectarse.

Si quieres probar en Docker Swarm (requisito de entrega), configura `--bootstrap` con las direcciones de servicio de las instancias y exp√≥n el puerto overlay.

## Limitaciones actuales

- Gossip es full-push: env√≠a la lista completa de providers por infoHash a cada peer bootstrap. No es escalable a escala masiva, pero es claro y suficiente para la 2¬™ entrega (prototipo).
- No hay autenticaci√≥n ni firma en announces. Un nodo malicioso podr√≠a anunciar providers falsos. Recomendado: validaci√≥n al intentar conectar (ya se verifica por SHA-1 en `peerwire`) y uso de HMAC/pubkeys si se desea seguridad.
- No hay NAT traversal: para que peers se conecten, `--hostname` debe ser una direcci√≥n alcanzable desde los otros nodos (o usar mapeo/port-forwarding en Docker).
- Sin delete expl√≠cito: eliminamos entries por TTL.

## C√≥mo encaja con `instructions.md` (requisitos de la 2¬™ entrega)

- Organizaci√≥n y roles: cada nodo ejecuta cliente + overlay. No existe punto central: la informaci√≥n se replica en todos los nodos que formen la red.
- Procesos: el sistema tiene procesos de cliente BitTorrent y un proceso overlay (listener TCP + periodic gossip + handlers). Ambos pueden correr en la misma instancia/container.
- Comunicaci√≥n: gossip sobre TCP JSON para control/descubrimiento; peerwire sigue usando conexiones peer-to-peer para transferencia de piezas.
- Coordinaci√≥n: decisiones distribuidas por r√©plica y anti-entropy (merge LWW). Para tareas que requieran exclusividad puede a√±adirse una elecci√≥n (leader election ligera) m√°s adelante.
- Nombrado/Localizaci√≥n: recurso identificado por `infoHash`. Localizaci√≥n mediante providers replicados en el store. Lookup realiza consultas a vecinos si la informaci√≥n local es insuficiente.
- Consistencia y replicaci√≥n: eventual consistency mediante gossip; Merge LWW; TTL y anti-entropy permiten convergencia.
- Tolerancia a fallas: sin tracker central, la ca√≠da de nodos no detiene la localizaci√≥n si hay r√©plicas; el sistema tolera nodos ca√≠dos siempre que exista r√©plica suficiente.
- Seguridad: pendiente (ver Limitaciones). Para la entrega, justificar√°s validaci√≥n en transferencia (SHA-1) y pol√≠ticas de reputaci√≥n como mitigaci√≥n.

## Tests y verificaci√≥n

- Compilaci√≥n: `cd src && go build ./...` (ya verificado localmente durante la implementaci√≥n).
- Test unitarios pendientes: crear tests para `Store.Merge`, TTL eviction y `Lookup`.
- Integration tests: probar con 3 instancias en CI/local con goroutines o contenedores.

## Pr√≥ximos pasos recomendados (prioridad)

1. A√±adir tests unitarios para `src/overlay` (merge, announce, lookup, TTL).  
2. Implementar `leave` o tombstones y/o un mecanismo expl√≠cito para remover providers.  
3. Optimizar gossip (push/pull diffs, bloom filters) para reducir ancho de banda.  
4. A√±adir validaci√≥n/handshake adicional en overlay y opcional HMAC para mitigaci√≥n de falsos announces.  
5. Documentar despliegue en Docker Swarm (variables `--bootstrap` y c√≥mo exponer `--overlay-port` entre hosts).

## Resumen de los cambios (lista r√°pida)

- A√±adido: `src/overlay/store.go` (Store, ProviderMeta).
- A√±adido: `src/overlay/gossip.go` (Overlay, TCP JSON gossip, Announce/Lookup).
- Modificado: `src/client/cmd/main.go` ‚Äî integr√© `--discovery-mode=overlay` y flags `--bootstrap`, `--overlay-port`, y encamin√© announces/lookups al overlay cuando est√° activo.

Si quieres, puedo:
- A√±adir tests unitarios e integrados ahora.
- Implementar optimizaciones de gossip (diffs/bloom) y/o `leave`/tombstones.
- Generar el informe final formateado para la 2¬™ entrega rellenando las secciones de `instructions.md` con el dise√±o y decisiones.

---
Archivo generado autom√°ticamente que documenta la implementaci√≥n del overlay gossip del repositorio.



# üö® Problemas actuales de tu overlay (versi√≥n ‚Äúbootstrap fijo + full-push gossip‚Äù)
1. Dependencia r√≠gida de peers bootstrap (single point of failure encubierto)

Tu cliente:

- Arranca solo con la lista est√°tica --bootstrap.

- Nunca aprende otros nodos.

- Todos los gossip y lookups van siempre a los mismos.

Qu√© pasa:
Si los bootstrap:

- se caen,

- pierden red,

- o simplemente no responden‚Ä¶

‚û°Ô∏è tu descubrimiento se muere.
El overlay no ‚Äúse regenera‚Äù. No hay forma de encontrar m√°s peers.

2. Red con topolog√≠a est√°tica

Cada nodo est√° conectado solo a esos bootstrap.

Efecto pr√°ctico:

- No hay conectividad redundante.

- No hay malla.

- No crece la red entre nodos nuevos.

- La informaci√≥n no se propaga m√°s all√° del vecindario inicial.

- Resultado: gossip limitado, convergencia lenta o incompleta.

3. Gossip full-push no escala

Cada X segundos, cada nodo env√≠a todos los providers por todos los infohashes a todos los bootstrap.

- Peque√±a red ‚Üí funciona.
- Red de 50‚Äì100 nodos ‚Üí saturas ancho de banda y CPU.

- No es tolerancia a fallos, pero s√≠ afecta la disponibilidad.

4. No hay detecci√≥n activa de fallos

No hay:

- health checks

- heartbeats

- timeouts agresivos

- eviction de nodos muertos

Solo TTL para providers, no para peers.

Si un bootstrap muere, queda ah√≠ para siempre, ocupando slot in√∫til.

5. Lookup parcial y con baja cobertura

Lookup():

usa solo los bootstrap. Pregunta solo a 3 nodos y no itera

Si la info no est√° en ese set ‚Üí te devuelve vac√≠o aunque exista en la red.

6. No hay descubrimiento progresivo

Los nodos no comparten su propia lista de peers.

Eso mata:

- recuperaci√≥n ante fallos

- expansi√≥n del overlay

- resiliencia ante churn

# üõ†Ô∏è Soluciones claras (sin reescribir tu overlay entero)
## ‚úÖ 1. Kademlia-lite: lista din√°mica de peers

Implementa un ‚Äúbucket‚Äù simple:

- cada nodo mantiene hasta K = 20 peers

- si un bootstrap responde ‚Üí lo mantienes

- si no responde ‚Üí lo eliminas

- cuando recibes gossip/lookup ‚Üí agregas a quien te contacta

Resultado:
üîß La red se vuelve autoexpandible.
üí™ Si un bootstrap muere, el nodo ya aprendi√≥ otros peers.

## ‚úÖ 2. Lookup iterativo (semi-Kademlia)

Tu lookup actual:

- local ‚Üí bootstrap ‚Üí done


Mejor:

- local ‚Üí peers m√°s cercanos ‚Üí nuevos peers ‚Üí repetir


Hasta que no haya nodos ‚Äúm√°s cercanos‚Äù al infoHash.

Esto garantiza que encontras al proveedor si existe (convergencia).

## ‚úÖ 3. Gossip push-pull con diffs (reduces carga + mejora convergencia)

En vez de mandar TODO cada vez:

- env√≠as resumen (hashes / timestamps)

- si el receptor detecta diferencias ‚Üí pide solo lo nuevo

Beneficios:

- convergencia m√°s r√°pida

- menos ancho de banda

- menos necesidad de depender de bootstrap

## ‚úÖ 4. ‚ÄúPeer exchange‚Äù al estilo BitTorrent

Cuando haces gossip/lookup, agrega en la respuesta:

```json
"peers": [listado de peers del receptor]
```

Cada nodo va aprendiendo nuevos peers naturalmente.

## ‚úÖ 5. Health checks simples

Cuando un peer falla 2‚Äì3 veces:

- se elimina del bucket

- se reemplaza con nodos nuevos que vayas aprendiendo

- Esto evita ‚Äúlista podrida‚Äù.

## ‚úÖ 6. Bootstrap redundante

Permitir:

- 5‚Äì10 direcciones bootstrap que reconstruyan la red inicial

Pero luego:
no deben ser obligatorios gracias al bucket din√°mico.

# üß® Resumen duro

1. Problemas actuales:

- Dependencia absoluta en bootstrap (si caen, muere el descubrimiento).

- Topolog√≠a fija, la red no se autoexpande.

- Gossip excesivo y sin optimizaci√≥n.

- Lookup incompleto (no explora la red).

- Sin mecanismos para aprender nuevos peers.

- Ausencia de tolerancia a fallos de nodos.

2. Soluciones recomendadas:

- Bucket din√°mico estilo Kademlia-lite.

- Lookup iterativo con vecinos ‚Äúm√°s cercanos‚Äù.

- Gossip push-pull con diffs.

- Peer Exchange en respuestas.

- Health checks + eviction de nodos muertos.

- Bootstrap solo como arranque, no como dependencia permanente.